---
layout: post
title: "torch.nn.Embedding 단위 학습"
date: 2024-08-14 +0900
categories: [pytorch, text]
tags: [python, pythorch, text]
published: true
author: jihwan
toc: true
---

## 기본 개념

- torch.nn.Embedding은 단어, 아이템, 또는 기타 범주형 데이터를 고정된 크기의 벡터 공간으로 매핑하는데 사용되는 레이어
- 하나의 큰 임베딩 행렬을 학습하는 레이어로 각 입력 인덱스에 대해 고정된 크기의 벡터를 반환한다. 
- torch.nn.Embedding으로 만들어진 임베딩 레이어는 0이상 정수로 이루어진 단어 인덱스를 입력으로 받는다. 

## 예시 1
```python

# 단어 집합의 크기는 10, 임베딩 벡터의 크기는 100으로 설정
embedding = nn.Embedding(num_embeddings = 10, embedding_dim = 100)

# 단어 집합의 크기인 0~9까지 숫지를 20개 생성
inputs = torch.randint(low=0, high=10, size=(2,))
# tensor([5, 4, 7, 6, 4, 3, 2, 1, 4, 5, 2, 5, 5, 5, 7, 8, 5, 7, 3, 4])

output = embedding(inputs[:2])
# tensor([[-0.1311,  0.1388, -0.9045, -2.1922,  0.4182, -0.7276,  0.1815, -0.8695,
#           1.7971, -1.3688, -0.3573,  0.3568, -1.0662, -1.6511,  0.2336,  0.1912,
#          -1.3698,  0.1815,  0.0210, -0.8856, -0.8091, -0.9547,  0.9409,  0.8585,
#           0.8842,  1.4731, -0.0280,  0.3825, -1.4255, -1.5557,  1.5898,  0.7847,
#          -1.0047, -2.0978, -0.0046,  0.7555,  0.3647,  0.8707,  1.6667, -1.1207,
#          -0.0820,  0.2143, -0.2111,  0.1378, -0.2491,  0.4755,  0.9535, -1.0736,
#           0.4308,  0.3873,  0.7024, -0.9198,  0.5606, -0.6396, -0.9648,  0.6614,
#           1.1598,  0.1886, -1.6506, -0.5899,  0.9722, -1.7994, -0.7413, -1.7660,
#          -0.1960,  0.3664,  1.4557, -0.1224, -0.3871,  1.2520, -1.3546, -2.1380,
#          -1.0249,  1.0715, -1.5270,  1.8277, -0.1266, -0.5641, -2.2811,  2.3142,
#           0.6057,  0.7716, -0.1112, -0.4410, -0.8084, -0.7178,  0.2184, -1.7888,
#           0.3551,  1.4991, -1.2022, -0.6240, -0.0265,  0.3173, -0.6419,  0.3778,
#          -1.0876, -0.4400,  0.4894, -0.4877],
#         [-0.2491, -0.4883, -0.0424,  0.8694,  0.1322, -0.5274,  0.6111, -0.7220,
#          -0.8014,  0.1306,  0.6015,  1.5844,  0.1061,  1.3465,  2.3454,  0.5750,
#           0.7527,  0.8106, -0.0749, -0.3041,  0.2728, -2.0881, -1.4789,  0.0991,
#          -1.3591,  0.6641, -0.3814, -0.8539, -0.6236,  2.0108, -1.6803,  0.1162,
#           1.5140,  0.0072,  0.7272,  0.4606,  2.0413,  0.5664, -0.0829, -0.8841,
#           0.7020,  0.7515,  1.8766, -0.1265, -1.0224,  0.4389,  0.2269, -0.1698,
#           1.0172, -1.7664,  0.0409,  0.0236, -0.3409, -1.4525,  0.3048,  0.1436,
#           0.3037,  0.9805,  0.7035, -0.1964, -2.0277, -1.5199,  1.4811, -0.9599,
#          -0.8666, -2.1072,  0.2290,  0.9316, -0.2740,  0.9431,  0.6028,  0.2100,
#           0.3542, -0.7151,  0.0780,  0.9690, -0.1032, -0.1671,  0.3951, -1.0037,
#          -1.6403,  0.6033,  0.6597,  1.8518,  1.3051, -0.7070, -0.7024, -0.3232,
#           1.5878,  0.8467, -0.0370,  0.4142,  0.0067,  1.1681, -0.0770,  1.7293,
#           1.8425,  0.1421, -0.8666, -0.4556]], grad_fn=<EmbeddingBackward0>)
```


## 예시 2
```python
import torch
import torch.nn as nn

# 단어 집합의 크기는 10, 임베딩 벡터의 크기는 3으로 설정
embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)

# 임의의 입력 텐서 생성 (각 값은 단어 인덱스)
input = torch.tensor([1, 2, 3, 4])

# 임베딩 레이어를 통해 인덱스를 임베딩 벡터로 변환
# input.shape = torch.Size([4])
output = embedding(input)

print(output)
# tensor([[-0.8063,  0.5854, -0.7533],
#         [ 0.1299, -1.2406,  0.0246],
#         [ 1.0147, -0.0055, -1.9632],
#         [ 1.1001, -1.0474, -1.4842]], grad_fn=<EmbeddingBackward0>)
```